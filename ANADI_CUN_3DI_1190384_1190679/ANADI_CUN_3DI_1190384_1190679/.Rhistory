#α=0.05
res.nn<- nn.pred - ciclistas.test$vo2_results
lillie.mlr <- lillie.test(residuals(mlr.model))
lillie.nn <- lillie.test(res.nn)
lillie.mlr
lillie.nn
#Como ambos o p-value de ambos os testes Shapiro são superiores ao
#nivel de significância, aceita-se a hipóste nula, e conclui-se que ambos os
#modelos seguem uma distribuição normal.
#Com isto, irá ser preciso recorrer a testes paramétricos, utilizando-se
#o t test
#H0:regressão linear multipla = Rede Neuronal
#H1:regressão linear multipla != Rede Neuronal
#α=0.05
v.mlr <- c(mlr.mae, mlr.rmse)
v.nn <- c(nn.mae, nn.rmse)
t.test(v.mlr, v.nn)
#Como o p-value = 0.9027 é superior a 0.05, não podemos rejeitar a hipótese nula,
#não existem evidencias estatistica com o nivel de significancia de 5% que provem que
#existe diferenças significativas entre a precisão dos dois modelos
#Tópico 4.2
#Exercicio 1
###Árvore de decisão
sample = sample(1:nrow(ciclistas), 0.7 * nrow(ciclistas))
ciclistas.train = ciclistas[sample,]
ciclistas.test = ciclistas[-sample,]
tree.model = rpart(Pro.level ~ . , data = ciclistas.train)
rpart.plot(
tree.model,
digits = 3,
fallen.leaves = TRUE,
type = 3
)
tree.pred = predict(tree.model, ciclistas.test, type = 'matrix')
m_conf_arvore_dec <- table(ciclistas.test$Pro.level, tree.pred)
acc_arvore_dec <- sum(diag(m_conf_arvore_dec))/sum(m_conf_arvore_dec)
cat("Accuracy: ", acc_arvore_dec * 100, "%") #Accuracy:  1.33333 %
#Rede neuronal
ciclistas_norm = as.data.frame(lapply (ciclistas, minmax))
treino_norm = as.data.frame(lapply (ciclistas.train, minmax))
teste_norm = as.data.frame(lapply (ciclistas.test, minmax))
#criacao da rede com 2 niveis internos de 1 no cada um
nn.model <- neuralnet(Pro.level ~ ., data = treino_norm, hidden = c(1,1))
plot(nn.model)
#Ver qual a accuracy
prevs_modelo_rede <- predict(nn.model, ciclistas.test, type = "class")
m_conf_rede <- table(ciclistas.test$Pro.level,prevs_modelo_rede)
accuracy_rede <- sum(diag(m_conf_rede))/sum(m_conf_rede)
cat("Accuracy: ", accuracy_rede * 100, "%") #Accuracy:  6.3333 %
#K-VIZINHOS MAIS PRÓXIMOS
ncols <- dim(ciclistas.norm)[2] #numero de colunas -> 10
treino_norm <- ciclistas.norm[index, -ncols]
teste_norm <- ciclistas.norm[-index, -ncols]
treino_norm.Pro.Level <- ciclistas.norm[index, ncols]
teste_norm.Pro.Level <- ciclistas.norm[-index, ncols]
k <- c()
accuracy <- c()
for(i in seq (1,50,2)){
knn.pred <- knn(train = treino_norm , test = teste_norm , cl = treino_norm.Pro.Level , k= i)
m.conf <- table(teste_norm.Pro.Level, knn.pred)
accuracy <- c(accuracy, sum(diag(m.conf))/sum(m.conf))
k <- c(k,i)
}
k_max <- k[which.max(accuracy)]
k_max # 11
plot(k, accuracy)
knn.pred <- knn(train = treino_norm , test = teste_norm , cl = treino_norm.Pro.Level , k_max)
m_conf_k_viz <- table(ciclistas.test$Pro.level,knn.pred)
acc_k_viz <- sum(diag(m_conf_k_viz))/sum(m_conf_k_viz)
cat("Accuracy: ", acc_k_viz * 100, "%") #Accuracy:4.666667  %
##OS MELHORES MÈTODOS SÃO A ÁRVORE DE DECISÃO E O  K-VIZINHOS MAIS PRÓXIMOS
##a
kf <- 10
folds <- sample(1: kf, nrow (ciclistas), replace = TRUE)
cv.error <- matrix(nrow = kf , ncol = 2)
k <- k_max #39
for (i in 1: kf){
train.cv <- ciclistas.norm[folds != i,]
test.cv <- ciclistas.norm[folds == i,]
ncols <- dim(train.cv)[2]
train.Ativo <- ciclistas.norm[folds != i, ncols ]
tst.Ativo <- ciclistas.norm[folds == i, ncols]
knn.pred <- knn (train = train.cv[, -ncols], test = test.cv[,-ncols], cl= train.Ativo , k= k_max)
m.conf <- table(tst.Ativo,knn.pred)
nn.model <- neuralnet(Pro.level ~ ., data = train.cv)
prevs_modelo_rede <- predict(nn.model, test.cv, type = "class")
m_conf_rede <- table(tst.Ativo,prevs_modelo_rede)
cv.error [i, ] <- c (sum (diag (m.conf))/ sum (m.conf), sum(diag(m_conf_rede))/sum(m_conf_rede))
}
apply(cv.error, 2, mean) #0.043645815(rede) e 0.009311413(knn)
apply(cv.error, 2, sd) #0.022955728 (rede) e 0.008410984(knn)
#c.
#H0:  nao há diferenças significativas vs H1: há diferenças significativas
teste <- t.test(cv.error[1,], cv.error[2,])
teste$p.value # 0.6847019
#Como o p = 0.6847019 e , portanto, é superior a 0.05,  não podemos rejeitar a hipotese nula.
#Não existem evidências estatísticas, ao nível de significância de 5%, que provem que existem
#diferencas significativas entre a accuracy dos dois modelos
#d.
#Rede neuronal
print(m_conf_rede)
#Accuracy
accuracy_rede #0.16
#Precision = Sensitivity = TP/(TP+FP)
TP_rede <- m_conf_rede[1,1] #0
FP_rede <- m_conf_rede[2,1] #0
precision_rede <- (TP_rede/(TP_rede+FP_rede))
precision_rede #NaN
#Recall 0 Specificity = TP/(TP+FN)
FN_rede <- m_conf_rede[1,2]
recall_rede <-(TP_rede/(TP_rede+FP_rede))
recall_rede #NaN
#F1 = ((2*precision*recall)/(precision+recall))
F1_rede <- ((2*precision_rede*recall_rede)/(precision_rede+recall_rede))
F1_rede #NaN
#K-vizinhos-mais-proximos
print(m_conf_k_viz)
#Accuracy
acc_k_viz #0.04
#Precision = Sensitivity = TP/(TP+FP)
TP_k_viz <- m_conf_k_viz[1,1] #2
FP_k_viz <- m_conf_k_viz[2,1] #0
precision_k_viz <- (TP_k_viz/(TP_k_viz+FP_k_viz))
precision_k_viz #0.2222222
#Recall = Specificity = TP/(TP+FN)
FN_k_viz <- m_conf_k_viz[1,2]
recall_k_viz <-(TP_k_viz/(TP_k_viz+FN_k_viz))
recall_k_viz #0.2857143
#F1 = ((2*precision*recall)/(precision+recall))
F1_k_viz <- ((2*precision_k_viz*recall_k_viz)/(precision_k_viz+recall_k_viz))
F1_k_viz #0.25
#RESUMO
accuracy_rede #0.16
acc_k_viz #0.04
precision_rede #NaN
precision_k_viz #0.2222222
recall_rede #NaN
recall_k_viz #0.2857143
F1_rede #NaN
F1_k_viz #0.25
#O modelo que apresentou melhor desempenho de acordo com os criterios foi a Rede Neuronal
#Exercicio 2
#Árvore de Decisão
set.seed(123)
index2 <- sample(1:nrow(ciclistas.norm), 0.7 * nrow(ciclistas.norm))
ciclistas.train <- ciclistas.norm[index2, -11]
ciclistas.test <- ciclistas.norm[-index2, -11]
summary(ciclistas.train)
train_labels <- ciclistas.norm[index2, "Winter.Training.Camp"]
tst_labels <- ciclistas.norm[-index2, "Winter.Training.Camp"]
tree.model2 <- rpart(Winter.Training.Camp ~ ., method = "class", data = ciclistas.train)
# Visualizar a árvore de classificação
rpart.plot(tree.model2, digits = 3)
# Previsões discretas
rpart.pred <- predict(tree.model2, ciclistas.test, type = "class")
rpart.pred
# Matriz de confusão da árvore de decisão
m_conf_arvore <- table(tst_labels, rpart.pred)
print(m_conf_arvore)
# Cálculo da precisão (accuracy) da árvore de decisão
accuracy_arvore <- sum(diag(m_conf_arvore)) / sum(m_conf_arvore)
accuracy_arvore # 0.6966667
# Rede Neural
model2 <- as.formula("Winter.Training.Camp ~ gender + Team + Background + Pro.level + altitude_results + vo2_results + hr_results + Continent + Age")
numnodes2 <- 1
nn.model2 <- neuralnet(model2, data = ciclistas.train, hidden = numnodes2)
# Visualizar a estrutura da rede neural
plot(nn.model2)
# Previsões da rede neural
prevs_modelo_rede <- compute(nn.model2, ciclistas.test[, -which(names(ciclistas.test) == "Winter.Training.Camp")])$net.result
# Arredondar as previsões para valores binários
rede.pred <- ifelse(prevs_modelo_rede > 0.5, 1, 0)
# Matriz de confusão da rede neural
m_conf_rede <- table(tst_labels, rede.pred)
# Cálculo da precisão (accuracy) da rede neural
accuracy_rede <- mean(rede.pred == ciclistas.test$Winter.Training.Camp)
accuracy_rede # 0.7066667
#a) Calcular média e o desvio padrão usando o método k-fold cross validation
data <- ciclistas.norm
k <- 10
folds <- sample(1:k, nrow(data), replace = TRUE)
numnodes <- 1
# Matriz para armazenar as precisões
accuracy <- matrix(nrow = k, ncol = 2)
# Loop k-fold cross validation
for (i in 1:k) {
train.cv <- data[folds != i, ]
test.cv <- data[folds == i, ]
train.Camp <- data[folds != i, "Winter.Training.Camp"]
tst.Camp <- data[folds == i, "Winter.Training.Camp"]
# Modelo de rede neural
nn.model <- neuralnet(model2, data = train.cv, hidden = numnodes)
prevs_modelo_rede <- compute(nn.model, test.cv[, -which(names(test.cv) == "Winter.Training.Camp")])$net.result
nn.pred <- ifelse(prevs_modelo_rede > 0.5, 1, 0)
# Matriz de confusão da rede neural
cfmatnn <- table(tst.Camp, nn.pred)
# Modelo de árvore de decisão
tree.model <- rpart(Winter.Training.Camp ~ ., method = "class", data = train.cv)
rpart.pred <- predict(tree.model, test.cv, type = "class")
# Matriz de confusão da árvore de decisão
cfmatrpart <- table(tst.Camp, rpart.pred)
# Armazenar as precisões na matriz accuracy
accuracy[i, ] <- c(sum(diag(cfmatnn)) / sum(cfmatnn),
sum(diag(cfmatrpart)) / sum(cfmatrpart))
}
# Precisão média
mean_accuracy <- apply(accuracy, 2, mean)
cat("Precisão média (nn e árvore):", mean_accuracy, "\n") # rede neuronal:0.7323471  arvore:0.7015413
# Desvio padrão
sd_accuracy <- apply(accuracy, 2, sd)
cat("Desvio padrão (nn e árvore):", sd_accuracy, "\n") # rede neuronal:0.0398576  arvore:0.04000683
#b) Verificar se existe diferença significativa no desempenho dos dois melhores modelos obtidos
ciclistas.nn<- rede.pred - ciclistas.test$Winter.Training.Camp
ciclistas.tree <- rpart.pred
summary(rpart.pred)
# Verificar normalidade dos resíduos do modelo de rede neural
lillie.nn <- lillie.test(ciclistas.nn)
lillie.nn
#Lilliefors (Kolmogorov-Smirnov) normality test
#D = 0.38306, p-value < 2.2e-16
# Teste de hipótese para comparar a diferença nas precisões
test <- wilcox.test(accuracy[, 1], accuracy[, 2])
test
#Wilcoxon rank sum test with continuity correction
#W = 67, p-value = 0.2176
#c) Comparar resultados dos modelos
# Cálculo das métricas de avaliação para o modelo de árvore de decisão
TP_arvore <- m_conf_arvore[1, 1]
FP_arvore <- m_conf_arvore[2, 1]
FN_arvore <- m_conf_arvore[1, 2]
accuracy_arvore <- sum(diag(m_conf_arvore)) / sum(m_conf_arvore)
precision_arvore <- TP_arvore / (TP_arvore + FP_arvore)
recall_arvore <- TP_arvore / (TP_arvore + FN_arvore)
F1_arvore <- (2 * precision_arvore * recall_arvore) / (precision_arvore + recall_arvore)
# Cálculo das métricas de avaliação para o modelo de rede neural
TP_rede <- m_conf_rede[1, 1]
FP_rede <- m_conf_rede[2, 1]
FN_rede <- m_conf_rede[1, 2]
accuracy_rede <- sum(diag(m_conf_rede)) / sum(m_conf_rede)
precision_rede <- TP_rede / (TP_rede + FP_rede)
recall_rede <- TP_rede / (TP_rede + FN_rede)
F1_rede <- (2 * precision_rede * recall_rede) / (precision_rede + recall_rede)
# Resultados Finais
accuracy_arvore # 0.7066667
accuracy_rede # 0.7233333
precision_arvore # 0.7
precision_rede # 0.7268722
recall_arvore # 0.9162304
recall_rede # 0.8638743
F1_arvore # 0.7936508
F1_rede # 0.7894737
#Exercicio 3
#a)
# Definir o número de folds para o k-fold cross-validation
k <- 10
# Criar os folds para o cross-validation
folds <- createFolds(ciclistas.norm$gender, k = k)
# Definir as métricas de avaliação
metric <- "Accuracy"
# Vetor para armazenar as taxas de acerto (accuracy)
accuracy_rede <- numeric(k)
accuracy_knn <- numeric(k)
# Loop para realizar o k-fold cross-validation
for (i in 1:k) {
# Dividir os dados em conjunto de treinamento e teste para o fold atual
train_data <- ciclistas.norm[-folds[[i]], ]
test_data <- ciclistas.norm[folds[[i]], ]
# Treinar o modelo de rede neural
nn_model <- neuralnet(model, data = train_data, hidden = numnodes)
# Fazer previsões usando o modelo de rede neural
nn_pred <- compute(nn_model, test_data)$net.result
nn_pred <- minmaxdesnorm(nn_pred, ciclistas.norm$gender)
# Calcular a taxa de acerto para o modelo de rede neural
accuracy_rede[i] <- sum(nn_pred == test_data$gender) / length(test_data$gender)
# Treinar o modelo de KNN
knn_model <- train(gender ~ ., data = train_data, method = "knn")
# Fazer previsões usando o modelo de KNN
knn_pred <- predict(knn_model, newdata = test_data)
# Calcular a taxa de acerto para o modelo de KNN
accuracy_knn[i] <- sum(knn_pred == test_data$gender) / length(test_data$gender)
}
# Calcular a média e o desvio padrão da taxa de acerto para ambos os modelos
mean_accuracy_rede <- mean(accuracy_rede)
sd_accuracy_rede <- sd(accuracy_rede)
mean_accuracy_knn <- mean(accuracy_knn)
sd_accuracy_knn <- sd(accuracy_knn)
# Exibir os resultados
cat("Rede Neural - Taxa de Acerto Média:", mean_accuracy_rede, "\n")
cat("Rede Neural - Desvio Padrão da Taxa de Acerto:", sd_accuracy_rede, "\n")
cat("KNN - Taxa de Acerto Média:", mean_accuracy_knn, "\n")
cat("KNN - Desvio Padrão da Taxa de Acerto:", sd_accuracy_knn, "\n")
test <- wilcox.test(accuracy_rede, accuracy_knn, paired = TRUE)
# Verificar o resultado do teste
p_value <- test$p.value
# Exibir o resultado
if (p_value < 0.05) {
cat("Existe diferença significativa no desempenho dos modelos (p < 0.05)")
} else {
cat("Não existe diferença significativa no desempenho dos modelos (p >= 0.05)")
}
p_value
# Calcular as métricas para o modelo de rede neural
TP_rede <- sum(m_conf_rede[1, 1])
FP_rede <- sum(m_conf_rede[2, 1])
TN_rede <- sum(m_conf_rede[2, 2])
FN_rede <- sum(m_conf_rede[1, 2])
accuracy_rede <- (TP_rede + TN_rede) / (TP_rede + FP_rede + TN_rede + FN_rede)
sensitivity_rede <- TP_rede / (TP_rede + FN_rede)
specificity_rede <- TN_rede / (TN_rede + FP_rede)
F1_rede <- (2 * TP_rede) / (2 * TP_rede + FP_rede + FN_rede)
# Calcular as métricas para o modelo de KNN
TP_knn <- sum(m_conf_knn[1, 1])
k <- 10
# Criar os folds para o cross-validation
folds <- createFolds(ciclistas.norm$gender, k = k)
# Definir as métricas de avaliação
metric <- "Accuracy"
# Vetor para armazenar as taxas de acerto (accuracy)
accuracy_rede <- numeric(k)
accuracy_knn <- numeric(k)
# Loop para realizar o k-fold cross-validation
for (i in 1:k) {
# Dividir os dados em conjunto de treinamento e teste para o fold atual
train_data <- ciclistas.norm[-folds[[i]], ]
test_data <- ciclistas.norm[folds[[i]], ]
# Treinar o modelo de rede neural
nn_model <- neuralnet(model, data = train_data, hidden = numnodes)
# Fazer previsões usando o modelo de rede neural
nn_pred <- compute(nn_model, test_data)$net.result
nn_pred <- minmaxdesnorm(nn_pred, ciclistas.norm$gender)
# Calcular a taxa de acerto para o modelo de rede neural
accuracy_rede[i] <- sum(nn_pred == test_data$gender) / length(test_data$gender)
# Converter a variável de destino em um fator de dois níveis para o modelo de KNN
train_data$gender <- as.factor(train_data$gender)
# Treinar o modelo de KNN
knn_model <- train(gender ~ ., data = train_data, method = "knn")
# Fazer previsões usando o modelo de KNN
knn_pred <- predict(knn_model, newdata = test_data)
# Calcular a taxa de acerto para o modelo de KNN
accuracy_knn[i] <- sum(knn_pred == test_data$gender) / length(test_data$gender)
}
# Calcular a média e o desvio padrão da taxa de acerto para ambos os modelos
mean_accuracy_rede <- mean(accuracy_rede)
sd_accuracy_rede <- sd(accuracy_rede)
mean_accuracy_knn <- mean(accuracy_knn)
sd_accuracy_knn <- sd(accuracy_knn)
# Exibir os resultados
cat("Rede Neural - Taxa de Acerto Média:", mean_accuracy_rede, "\n")
cat("Rede Neural - Desvio Padrão da Taxa de Acerto:", sd_accuracy_rede, "\n")
cat("KNN - Taxa de Acerto Média:", mean_accuracy_knn, "\n")
cat("KNN - Desvio Padrão da Taxa de Acerto:", sd_accuracy_knn, "\n")
diff_accuracy <- accuracy_rede - accuracy_knn
# Verificar se há empates nas diferenças
if (any(diff_accuracy == 0)) {
# Adicionar pequeno ruído às diferenças para evitar empates
diff_accuracy <- diff_accuracy + runif(length(diff_accuracy), min = 1e-8, max = 1e-7)
}
# Realizar o teste de Wilcoxon pareado
test <- wilcox.test(accuracy_rede, accuracy_knn, paired = TRUE)
# Verificar o resultado do teste
p_value <- test$p.value
p_value # 0.007086405
# Exibir o resultado
if (p_value < 0.05) {
cat("Existe diferença significativa no desempenho dos modelos (p < 0.05)")
} else {
cat("Não existe diferença significativa no desempenho dos modelos (p >= 0.05)")
}
# Calcular as métricas para o modelo de rede neural
TP_rede <- sum(m_conf_rede[1, 1])
FP_rede <- sum(m_conf_rede[2, 1])
TN_rede <- sum(m_conf_rede[2, 2])
FN_rede <- sum(m_conf_rede[1, 2])
accuracy_rede <- (TP_rede + TN_rede) / (TP_rede + FP_rede + TN_rede + FN_rede)
sensitivity_rede <- TP_rede / (TP_rede + FN_rede)
specificity_rede <- TN_rede / (TN_rede + FP_rede)
F1_rede <- (2 * TP_rede) / (2 * TP_rede + FP_rede + FN_rede)
# Calcular as métricas para o modelo de KNN
TP_knn <- sum(m_conf_knn[1, 1])
model2 <- as.formula("Winter.Training.Camp ~ gender + Team + Background + Pro.level + altitude_results + vo2_results + hr_results + Continent + Age")
numnodes2 <- 1
nn.model2 <- neuralnet(model2, data = ciclistas.train, hidden = numnodes2)
# Visualizar a estrutura da rede neural
plot(nn.model2)
# Previsões da rede neural
prevs_modelo_rede <- compute(nn.model2, ciclistas.test[, -which(names(ciclistas.test) == "gender")])$net.result
# Arredondar as previsões para valores binários
rede.pred <- ifelse(prevs_modelo_rede > 0.5, 1, 0)
# Matriz de confusão da rede neural
m_conf_rede <- table(tst_labels, rede.pred)
accuracy_rede <- (TP_rede + TN_rede) / (TP_rede + FP_rede + TN_rede + FN_rede)
sensitivity_rede <- TP_rede / (TP_rede + FN_rede)
specificity_rede <- TN_rede / (TN_rede + FP_rede)
F1_rede <- (2 * TP_rede) / (2 * TP_rede + FP_rede + FN_rede)
# Calcular as métricas para o modelo de KNN
TP_knn <- sum(m_conf_rede[1, 1])
FP_knn <- sum(m_conf_rede[2, 1])
TN_knn <- sum(m_conf_rede[2, 2])
FN_knn <- sum(m_conf_rede[1, 2])
accuracy_knn <- (TP_knn + TN_knn) / (TP_knn + FP_knn + TN_knn + FN_knn)
sensitivity_knn <- TP_knn / (TP_knn + FN_knn)
specificity_knn <- TN_knn / (TN_knn + FP_knn)
F1_knn <- (2 * TP_knn) / (2 * TP_knn + FP_knn + FN_knn)
# Comparar os resultados dos modelos
results <- data.frame(Modelo = c("Rede Neural", "KNN"),
Accuracy = c(accuracy_rede, accuracy_knn),
Sensitivity = c(sensitivity_rede, sensitivity_knn),
Specificity = c(specificity_rede, specificity_knn),
F1 = c(F1_rede, F1_knn))
# Exibir os resultados
results
model2 <- as.formula("Winter.Training.Camp ~ gender + Team + Background + Pro.level + altitude_results + vo2_results + hr_results + Continent + Age")
numnodes2 <- 1
nn.model2 <- neuralnet(model2, data = ciclistas.train, hidden = numnodes2)
# Visualizar a estrutura da rede neural
plot(nn.model2)
# Previsões da rede neural
prevs_modelo_rede <- compute(nn.model2, ciclistas.test[, -which(names(ciclistas.test) == "gender")])$net.result
# Arredondar as previsões para valores binários
rede.pred <- ifelse(prevs_modelo_rede > 0.5, 1, 0)
# Matriz de confusão da rede neural
m_conf_rede <- table(tst_labels, rede.pred)
accuracy_rede <- (TP_rede + TN_rede) / (TP_rede + FP_rede + TN_rede + FN_rede)
sensitivity_rede <- TP_rede / (TP_rede + FN_rede)
specificity_rede <- TN_rede / (TN_rede + FP_rede)
F1_rede <- (2 * TP_rede) / (2 * TP_rede + FP_rede + FN_rede)
# Calcular as métricas para o modelo de KNN
TP_knn <- sum(m_conf_rede[1, 1])
FP_knn <- sum(m_conf_rede[2, 1])
TN_knn <- sum(m_conf_rede[2, 2])
FN_knn <- sum(m_conf_rede[1, 2])
accuracy_knn <- (TP_knn + TN_knn) / (TP_knn + FP_knn + TN_knn + FN_knn)
sensitivity_knn <- TP_knn / (TP_knn + FN_knn)
specificity_knn <- TN_knn / (TN_knn + FP_knn)
F1_knn <- (2 * TP_knn) / (2 * TP_knn + FP_knn + FN_knn)
# Comparar os resultados dos modelos
results <- data.frame(Modelo = c("Rede Neural", "KNN"),
Accuracy = c(accuracy_rede, accuracy_knn),
Sensitivity = c(sensitivity_rede, sensitivity_knn),
Specificity = c(specificity_rede, specificity_knn),
F1 = c(F1_rede, F1_knn))
# Exibir os resultados
results
#Organizar informação para criar matrix de confusão
model2 <- as.formula("Winter.Training.Camp ~ gender + Team + Background + Pro.level + altitude_results + vo2_results + hr_results + Continent + Age")
numnodes2 <- 1
nn.model2 <- neuralnet(model2, data = ciclistas.train, hidden = numnodes2)
#Packages
#install.packages("dplyr")
library(dplyr)
#install.packages("e1071")
library(e1071)
#install.packages("corrplot")
library(corrplot)
#install.packages("readr")
library(readr)
#install.packages("ggplot2")
library(ggplot2)
#install.packages("rpart")
library(rpart)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("neuralnet")
library(neuralnet)
#install.packages("NeuralNetTools")
library(NeuralNetTools)
#install.packages("FNN")
library(FNN)
#install.packages("class")
library(class)
#install.packages("car")
library(car)
#install.packages("nortest")
library(nortest)
#install.packages("caret")
library(caret)
#Organizar informação para criar matrix de confusão
model2 <- as.formula("Winter.Training.Camp ~ gender + Team + Background + Pro.level + altitude_results + vo2_results + hr_results + Continent + Age")
numnodes2 <- 1
nn.model2 <- neuralnet(model2, data = ciclistas.train, hidden = numnodes2)
# Visualizar a estrutura da rede neural
plot(nn.model2)
# Previsões da rede neural
prevs_modelo_rede <- compute(nn.model2, ciclistas.test[, -which(names(ciclistas.test) == "gender")])$net.result
# Arredondar as previsões para valores binários
rede.pred <- ifelse(prevs_modelo_rede > 0.5, 1, 0)
# Matriz de confusão da rede neural
m_conf_rede <- table(tst_labels, rede.pred)
accuracy_rede <- (TP_rede + TN_rede) / (TP_rede + FP_rede + TN_rede + FN_rede)
sensitivity_rede <- TP_rede / (TP_rede + FN_rede)
specificity_rede <- TN_rede / (TN_rede + FP_rede)
F1_rede <- (2 * TP_rede) / (2 * TP_rede + FP_rede + FN_rede)
# Calcular as métricas para o modelo de KNN
TP_knn <- sum(m_conf_rede[1, 1])
FP_knn <- sum(m_conf_rede[2, 1])
TN_knn <- sum(m_conf_rede[2, 2])
FN_knn <- sum(m_conf_rede[1, 2])
accuracy_knn <- (TP_knn + TN_knn) / (TP_knn + FP_knn + TN_knn + FN_knn)
sensitivity_knn <- TP_knn / (TP_knn + FN_knn)
specificity_knn <- TN_knn / (TN_knn + FP_knn)
F1_knn <- (2 * TP_knn) / (2 * TP_knn + FP_knn + FN_knn)
# Comparar os resultados dos modelos
results <- data.frame(Modelo = c("Rede Neural", "KNN"),
Accuracy = c(accuracy_rede, accuracy_knn),
Sensitivity = c(sensitivity_rede, sensitivity_knn),
Specificity = c(specificity_rede, specificity_knn),
F1 = c(F1_rede, F1_knn))
# Exibir os resultados
results
#       Modelo  Accuracy Sensitivity Specificity        F1
# 1 Rede Neural 0.7066667   0.8638743   0.4311927 0.7894737
# 2         KNN 0.6566667   0.7068063   0.5688073 0.7238606
